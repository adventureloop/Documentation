\section{Modern Graphics}
Dedicated hardware has been used for the creation of computer output since the
move away from blinking lights to indicate machine state. Graphics hardware has
been at the fore front of pushing computer power since the first 3D graphics 
cards started to appear in the early 90s and today graphics hardware still push
the limit of hardware even down into the mobile space.\\

Dedicated graphics hardware has allowed for a move from using the CPU of a 
computer to handle complex geometry tasks and as this has happened game designers
and developers have asked for more and more from the graphics chipset
manufacturers. The latest graphics cards are now more generalised vector 
processors that can be applied to many different data calculation intensive tasks
including the creation of complex, interesting and realistic 3D graphics.\\

\subsection{OpenGL Overview} 
Access to graphics needs to be mitigated through the use of libraries, these 
libraries have grown up with the graphics cards and together have pushed the 
limits of the hardware. Two competing standards have developed over time, Direct
3D from microsoft and the OpenGL specification for the ARB group. Direct 3D is 
limited to run only on microsoft platforms such as window and the xbox console,
but OpenGL is a standard that can be and is implemented everywhere. Due to this
Direct 3D has been discounted as an option for this project and instead OpenGL
has been investigated.\\

OpenGL is a specification for how graphics systems should operate, but also 
defines a software interface that should be implemented to control the operation
of the graphics hardware. As a specification a lot of the under lying 
implementation of the graphics system is left to the manufacturer, instead 
OpenGL defines the outputs that should appear from given inputs to the system,
the rest is a black box.\\

OpenGL's functionality is based around a state machine, this machine needs to be
configured by the program for each drawing call, but will preserve this state 
unless otherwise changed. For drawing OpenGL is a very powerful rasteriser, its
main opertaion being the processing of triangles in the form of vertices into
pixels on a final display. \\

OpenGL is only really a description of the interface with the hardware, for 
practical use the operating system must provide the window in which the final
display will appear. This allows the opengl commands to be platform agnostic, 
letting windowing functionality come from the OS itself.\\

\subsubsection*{Drawing}
A computer screen is a grid of squares called pixels(picture elements), each
pixel contains red green and blue lights and these are combined to create a 
colour in each pixel. 

A vector graphic is an image that is made up from mathematic formulae describing
the lines and colours in the image instead of the pixel data. Vector graphics
have the nice property that they will scall infinitely, you wont see any 
artifacts or large pixels when you zoom. Vector graphics are also easy to define
and very low on memory. Rasterisation is the process of converting a vector 
graphic into pixels that will be displayed on a screen. Graphics cards are 
pieces of hardware call raterisers that are specifically designed to stream line
this process of moving from a vector description of an image to final pixel 
values on the screen.\\

To build up realistic models using vector art graphics systems rely on a large
amount of fakery. OpenGL uses the triangle as its most primitive shape, these
triangles are then combined to create very complex shapes in what are called 
meshes. Any complex shape can be made from a number of triangles, the greater the
desired precision the more triangles are required.\\

\subsection{Rendering Pipeline}
\subsubsection{Triangles and Verticies}
OpenGL is the interface to the hardware rasteriser, A Vertex is the most 
primitive type that opengl can operate with. Each Vertex will describe its 
position in space and may include meta data about itself, like colour 
information. OpenGL has a number of different ways it can use vertex data, but 
the most common is to take vertices three at a time and use them to create 
triangles.\\

Once the vertex data for all the triangles in a mesh have been passed to OpenGL,
the information will be processed through the rasterisation pipeline. This 
pipeline has several discrete stages, each of which transforms the vertex data 
in some way until the final stage is reached and a colour can be decided for the
pixels in the window.\\

The rough stages for the rasterisation pipeline are:
\begin{description}
\item[Clip Space Transfromation] The first phase of the rasterisation process
is transformation of vertices into clip space. Clip space defines a region in 
space, anything inside of which will be rendered, anything outside of this space
will be disguared. Objects that lie across this boundary will have their surface
broken down into small triangles until all of which lie within clip space, hence
clipping. Positions in clip space have an additional W position, this defines the
extent of clip space for the coordinate in the [-W,W] range.
The operations in clip space can be quite arbitary as the programmer 
has a lot of control over how the clip transformation operates.

\item[Normalised Device Coordinates] The Normalised Device Coordinate space is 
used to make visualising triangles much simplier. It is effectivly the same as 
Clip space, but each coordinated lies within the [-1,1] range. The division is 
part of the projection from 3D shapes to 2D colours.
\item[Window Transformation] Window coordinated are converted to from normalised
device coordinates. In this space each coordinate is relative to the window that
we are rendering into, these coorinates are still 3D and have float point 
precision, window coordinates are not pixel mappings.
\item[Scan Converstion] Scan Conversion takes each triangle as described in 
window coordinated and maps it to pixels that are covered by its shape. Scan 
conversion will create a \emph{fragment} for each pixel sample that is within
the triangles shape.
\item[Fragment Processing] Fragment processing is the phase where each fragment 
is given one of more colours and has as depth value applied agasint it. This 
phase can also be qute arbitary as the programmer is given access to the pipeline
as this stage.
\item[Fragment Writing] The phase of the rasterisation pipeline is to write the 
coloured fragment to the display.
\end{description}

\subsection{Shaders}
There are two points in the rasterisation pipeline that have hooks allow 
customisation from the programmer. The first is during clip space transformation
and the second is during Fragment Processing. To access the rasteriation pipeline
small programs called shaders are used, these programs have hooks to access the
vertex data in the case of vertex shaders and access to the fragments created in
the case of the fragment shader.\\

OpenGL provides a simple C like language called GLSL(Graphics Library Shading 
Language), GLSL contains extensions that make writing programs that use vectors
and matrices very easy. Builtin variables are also provided to allow easier
implementation of shader programs. \\

\lstinputlisting{src/example.vsh}

Inside the vertex shader variable called attributes can be defined that directly
link to the vertex data provided by OpenGL. These variables are declared by 
placing the attribute specifier before the variable in the global block. Finally
uniforms variables can be passed to the shader programs, these values are the 
same for each rendering call, but can be changed between render calls.\\

Vertex shaders are used from geometric placement of vertices and similar tasks
that would involve a lot of processing on the cpu. These are be highly 
parrallelised on the GPU allows for more complex placement than was previously
possible. The vertex shader is also responsible for transforming vertices into
a view frustum, the visible view area. The vertex shader with also controll 
modelling and camera transformations and can be used in simple vertex based forms
of lighting.\\

The fragment shader is much more limited in its scope, it only has access to the
final fragments before they are written and any used specified uniforms. Despite
this the fragment shader is resposible for high resolution effects like fog and
many realistic forms of lighiting rely on the precision that is only available
within the fragment shader\\

\lstinputlisting{src/example.fsh}

There are many different types of graphics chipsets in the wild and it would be
difficult to create binaries that will run on every graphics card. To get around
this the compilation and linking of shader programs are performed at run time.
This allows our shader programs to run on almost any platform without many
changes.\\ 

\subsection{Programing with OpenGL}
OpenGL is a C based API which uses C function calls to access functionality on 
the graphics processor. The API is implemented as a collection of \#define'd 
constants, types and methods, all prefixed with GL 
(GLShort,GL\_DEPTH\_BUFFER\_BIT,glColor3f). OpenGL owns storage for all objects
used in the rendering, user programs have to request access to buffers and offer
memory regions to be used. Due to OpenGL's stateful nature contexts can be used 
in programs to make segmenting up drawing easier, an entity can configure it's 
drawing space, then save this in a context, stopping interferance from other 
entities drawing. \\

OpenGL is a simple API to a very complex underlying state machine, the API is 
very tolerant and provides a lot of the configuration for a global context by 
default. More complex programs need to do more work setting up, but allow the
programmer to create very powerful compartmentalised code that can be reused in 
many situations.\\

Before any drawing can be done, an OpenGL context has to be received from the OS
and the program needs to go through the process of loading and compiling the 
shader programs. For our example these are both abstracted away into functions as
they can be very OS dependant.  \\

\begin{description}
\item[Vertex Array Object] The first OpenGL based configurations is the 
creation of the buffer context and the allocation of memory on the graphics card
to be used. A vertex array object is used to store the locations of attributes 
and drawing related state, the location of the array data is also stored with
this object.
\item[Vertex Buffer Object]
To provide data to OpenGL a Vertex Buffer Object needs to be created
and assigned to the VAO, the VBO stores where in system memory the vertex data 
is. 
\item[Attribute Binding]
Once the VBO is aligned the program needs to bind the attributes it is going
to provide to the shader program, these are specified by an index given by their
order of definition inside the shader program.
\item[Attribute Pointer Assignment]
Finally the access types of the 
vertex data need to be configured, this is done by setting the attribute 
pointer. 
\end{description}

\begin{center}
\includegraphics[width=5cm]{Images/VertexArrays.png}
\end{center}

Vertex data can be packed as an individual array, or stored in an interwieved 
vertex array. This way all of the meta data about the vertex is stored in line 
relative to the vertex itself. This method allows a reduction in the number of 
memory buffers and simplifies specification of information. The 
glVertexAttribPointer function call makes this easy by providing a step value and
an initial offset value for memory locations. These are passed in the final two
parameters to the function call respectivly.\\

\lstinputlisting{src/progamablepipelineexample.c}

